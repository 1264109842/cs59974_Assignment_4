{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_4.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg9talP33urv"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1264109842/cs59974_Assignment_4/blob/main/Assignment_3.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8PKiVJaL_AW"
      },
      "source": [
        "# Imports and pip installations (if needed)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A05Q5B0_NUX9"
      },
      "source": [
        "# Part 1: Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 735
        },
        "id": "YZ4MUsbXNZ0P",
        "outputId": "77e7a628-792f-4d28-b7b1-e06f4db3efd3"
      },
      "source": [
        "# Load the dataset (load remotely, not locally)\n",
        "\n",
        "# Output the first 15 rows of the data\n",
        "# Display a summary of the table information (number of datapoints, etc.)\n",
        "\n",
        "# Load iris dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Display first 15 rows\n",
        "first_15 = iris.data[:15]\n",
        "\n",
        "# Display all classes of iris\n",
        "className = list(iris.target_names)\n",
        "\n",
        "# Display the name of each column\n",
        "columnName = list(iris.feature_names)\n",
        "\n",
        "# Number of datapoints\n",
        "totalDatapoints = len(iris.data)\n",
        "\n",
        "# Print the target name stored as label\n",
        "# Species: 0 = setosa, 1 = versicolor, 2 = virginica\n",
        "labels = list(iris.target)\n",
        "\n",
        "print('First 15 rows: ' , first_15)\n",
        "print('All class names: ', className)\n",
        "print('Column names: ', columnName)\n",
        "print('Total datapoints: ', totalDatapoints)\n",
        "print('Target: ', labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First 15 rows:  [[5.1 3.5 1.4 0.2]\n",
            " [4.9 3.  1.4 0.2]\n",
            " [4.7 3.2 1.3 0.2]\n",
            " [4.6 3.1 1.5 0.2]\n",
            " [5.  3.6 1.4 0.2]\n",
            " [5.4 3.9 1.7 0.4]\n",
            " [4.6 3.4 1.4 0.3]\n",
            " [5.  3.4 1.5 0.2]\n",
            " [4.4 2.9 1.4 0.2]\n",
            " [4.9 3.1 1.5 0.1]\n",
            " [5.4 3.7 1.5 0.2]\n",
            " [4.8 3.4 1.6 0.2]\n",
            " [4.8 3.  1.4 0.1]\n",
            " [4.3 3.  1.1 0.1]\n",
            " [5.8 4.  1.2 0.2]]\n",
            "All class names:  ['setosa', 'versicolor', 'virginica']\n",
            "Column names:  ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
            "Total datapoints:  150\n",
            "Target:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wRMtsJKBaxWu"
      },
      "source": [
        "## About the dataset\n",
        "Explain what the data is in your own words. What are your features and labels? What is the mapping of your labels to the actual classes?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkHTiTUd3ury"
      },
      "source": [
        "Explanation:\n",
        "    The dataset contains three classes of iris and their shapes. The features in iris dataset is speal length, speal width, petal length, and petal width. The labels of iris dataset is the number representation of each class that is setosa = 0, versicolor = 1, and virginica = 2. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhKaIrZKNaHg"
      },
      "source": [
        "# Part 2: Split the dataset into train and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrgogB62NcOi"
      },
      "source": [
        "# Take the dataset and split it into our features (X) and label (y)\n",
        "\n",
        "# Features\n",
        "X = iris.data\n",
        "\n",
        "# Label\n",
        "y = iris.target\n",
        "\n",
        "# Use sklearn to split the features and labels into a training/test set. (90% train, 10% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.9, test_size=0.1, random_state=42)\n",
        "\n",
        "# Convert training set and test set to a numpy array\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)\n",
        "X_test = np.array(X_test)\n",
        "y_test = np.array(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hblF-ei9Ncia"
      },
      "source": [
        "# Part 3: Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhFhEN03Nf7o",
        "outputId": "15a94661-020b-4520-b5f3-c34fdfb042f2"
      },
      "source": [
        "# i. Use sklearn to train a LogisticRegression model on the training set\n",
        "\n",
        "# Create a instance of logistic regression model\n",
        "logisticReg = LogisticRegression()\n",
        "\n",
        "# Train the model on training data\n",
        "logisticReg.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# ii. For a sample datapoint, predict the probabilities for each possible class\n",
        "\n",
        "# Create a sample datapoint\n",
        "sample = np.array([4.9, 2.1, 3.5, 1.1])\n",
        "\n",
        "# Predict possible calss of sample datapoint\n",
        "sample_predict = logisticReg.predict_proba(sample.reshape(1,-1))\n",
        "\n",
        "# # Rounding to the nearest hundredth\n",
        "sample_predict = np.round(sample_predict, 2)\n",
        "\n",
        "print('Sample datapoint: ', sample)\n",
        "print('The probability of setosa is: ' , sample_predict[:,0])\n",
        "print('The probability of versicolor is: ' , sample_predict[:,1])\n",
        "print('The probability of virginica is: ' , sample_predict[:,2])\n",
        "\n",
        "\n",
        "# iii. Report on the score for Logistic regression model, what does the score measure?\n",
        "\n",
        "#################################\n",
        "# the score \n",
        "\n",
        "# Get the mean accuracy of logistic regression model\n",
        "score = logisticReg.score(X_test, y_test)*100\n",
        "\n",
        "# Print the score\n",
        "print('\\nThe score of test data: %.f%%\\n' % score)\n",
        "\n",
        "# iv. Extract the coefficents and intercepts for the boundary line(s)\n",
        "\n",
        "# Get the coefficents\n",
        "coefficents = logisticReg.coef_\n",
        "\n",
        "# Get the intercept\n",
        "intercepts = logisticReg.intercept_\n",
        "\n",
        "# Print coefficents and intercept\n",
        "print('Coefficents: ', coefficents)\n",
        "print('Intercepts: ', intercepts)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample datapoint:  [4.9 2.1 3.5 1.1]\n",
            "The probability of setosa is:  [0.07]\n",
            "The probability of versicolor is:  [0.91]\n",
            "The probability of virginica is:  [0.01]\n",
            "\n",
            "The score of test data: 100%\n",
            "\n",
            "Coefficents:  [[-0.42685097  0.97283315 -2.44463796 -1.0318789 ]\n",
            " [ 0.51314447 -0.22362072 -0.21514806 -0.85146024]\n",
            " [-0.0862935  -0.74921243  2.65978602  1.88333914]]\n",
            "Intercepts:  [  9.50133366   1.91164253 -11.4129762 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDUpXQN4Nilk"
      },
      "source": [
        "# Part 4: Support Vector Machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U__zukpdNqiQ",
        "outputId": "5cd81a50-9137-4f36-842e-0687e9bc7c45"
      },
      "source": [
        "# i. Use sklearn to train a Support Vector Classifier on the training set\n",
        "\n",
        "# Train the training set on SVC use sklearn\n",
        "svc = make_pipeline(StandardScaler(), SVC(gamma='auto', probability=True))\n",
        "svc.fit(X_train, y_train)\n",
        "\n",
        "# ii. For a sample datapoint, predict the probabilities for each possible class\n",
        "\n",
        "# Create a sample datapoint\n",
        "sample = np.array([4.9, 2.1, 3.5, 1.1])\n",
        "\n",
        "# Predict the \n",
        "predict = svc.predict_proba([sample])\n",
        "\n",
        "# Rounding to the nearest hundredth\n",
        "sample_predict = np.round(predict, 2)\n",
        "\n",
        "# Print the probabilities of each class upon the datapoint\n",
        "print('Sample datapoint: ', sample)\n",
        "print('The probability of setosa is: ' , sample_predict[:,0])\n",
        "print('The probability of versicolor is: ' , sample_predict[:,1])\n",
        "print('The probability of virginica is: ' , sample_predict[:,2])\n",
        "\n",
        "# iii. Report on the score for the SVM, what does the score measure?\n",
        "\n",
        "# Get the mean accuracy of iris dataset on SVM model\n",
        "score = svc.score(X_test, y_test) * 100\n",
        "\n",
        "# Print the mean accuracy\n",
        "print('\\nThe score of test data: %.f%%' % score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample datapoint:  [4.9 2.1 3.5 1.1]\n",
            "The probability of setosa is:  [0.02]\n",
            "The probability of versicolor is:  [0.94]\n",
            "The probability of virginica is:  [0.04]\n",
            "\n",
            "The score of test data: 100%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULoL7mMBNrlS"
      },
      "source": [
        "# Part 5: Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKKmODVrN9lQ",
        "outputId": "dced24f7-c6bd-45fb-c68f-87548e9228dc"
      },
      "source": [
        "# i. Use sklearn to train a Neural Network (MLP Classifier) on the training set\n",
        "\n",
        "# Train the training set on MLP classifier\n",
        "mlp = MLPClassifier(max_iter=600,\n",
        "                    random_state=1).fit(X_train, y_train)\n",
        "\n",
        "# ii. For a sample datapoint, predict the probabilities for each possible class\n",
        "\n",
        "# Create a sample datapoint\n",
        "sample = [[4.9, 2.1, 3.5, 1.1]]\n",
        "\n",
        "# Predict the \n",
        "sample_predict = mlp.predict_proba(np.array(sample))\n",
        "\n",
        "# Rounding to the nearest thousandth\n",
        "sample_predict = np.round(sample_predict, 3)\n",
        "\n",
        "# Print the probabilities of each class upon the datapoint\n",
        "print('Sample datapoint: ', sample)\n",
        "print('The probability of setosa is: ' , sample_predict[:,0])\n",
        "print('The probability of versicolor is: ' , sample_predict[:,1])\n",
        "print('The probability of virginica is: ' , sample_predict[:,2])\n",
        "\n",
        "\n",
        "# iii. Report on the score for the Neural Network, what does the score measure?\n",
        "\n",
        "# Get the mean accuracy of iris dataset on SVM model\n",
        "score = mlp.score(X_test, y_test) * 100\n",
        "\n",
        "# Print the mean accuracy\n",
        "print('\\nThe score of test data: %.f%%' % score)\n",
        "\n",
        "# iv: Experiment with different options for the neural network, report on your best configuration (the highest score I was able to achieve was 0.8666)\n",
        "\n",
        "mlp = MLPClassifier(max_iter=3000,\n",
        "                    learning_rate_init = 0.002,\n",
        "                    solver='adam', \n",
        "                    hidden_layer_sizes=(6,), \n",
        "                    random_state=1).fit(X_train, y_train)\n",
        "\n",
        "# Get the mean accuracy of iris dataset on MLP Classifier model\n",
        "score = mlp.score(X_test, y_test) * 100\n",
        "\n",
        "# Print the mean accuracy\n",
        "print('\\nThe score of test data: %.f%%' % score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample datapoint:  [[4.9, 2.1, 3.5, 1.1]]\n",
            "The probability of setosa is:  [0.005]\n",
            "The probability of versicolor is:  [0.982]\n",
            "The probability of virginica is:  [0.014]\n",
            "\n",
            "The score of test data: 100%\n",
            "\n",
            "The score of test data: 93%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bi5tDwHmC28"
      },
      "source": [
        "# Part 6: K-Nearest Neighbors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCFlfJy2mCg6",
        "outputId": "e71bf88c-6418-4b7f-e289-4acf743c16cc"
      },
      "source": [
        "# i. Use sklearn to 'train' a k-Neighbors Classifier\n",
        "# Note: KNN is a nonparametric model and technically doesn't require training\n",
        "# fit will essentially load the data into the model see link below for more information\n",
        "# https://stats.stackexchange.com/questions/349842/why-do-we-need-to-fit-a-k-nearest-neighbors-classifier\n",
        "\n",
        "knc = KNeighborsClassifier(n_neighbors=3).fit(X_train, y_train)\n",
        "\n",
        "# ii. For a sample datapoint, predict the probabilities for each possible class\n",
        "\n",
        "# Create a sample datapoint\n",
        "sample = np.array([[4.9, 2.1, 3.5, 1.1]])\n",
        "\n",
        "# Predict the \n",
        "predict = knc.predict_proba(sample)\n",
        "\n",
        "# Rounding to the nearest hundredth\n",
        "sample_predict = np.round(predict, 2)\n",
        "\n",
        "# Print the probabilities of each class upon the datapoint\n",
        "print('Sample datapoint: ', sample)\n",
        "print('The probability of setosa is: ' , sample_predict[:,0])\n",
        "print('The probability of versicolor is: ' , sample_predict[:,1])\n",
        "print('The probability of virginica is: ' , sample_predict[:,2])\n",
        "\n",
        "# iii. Report on the score for kNN, what does the score measure?\n",
        "\n",
        "# Get the mean accuracy of iris dataset on KNN classifier model\n",
        "score = knc.score(X_test, y_test) * 100\n",
        "\n",
        "# Print the mean accuracy\n",
        "print('\\nThe score of test data: %.f%%' % score)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample datapoint:  [[4.9 2.1 3.5 1.1]]\n",
            "The probability of setosa is:  [0.]\n",
            "The probability of versicolor is:  [1.]\n",
            "The probability of virginica is:  [0.]\n",
            "\n",
            "The score of test data: 100%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "162Sw3LeoqE2"
      },
      "source": [
        "# Part 7: Conclusions and takeaways\n",
        "\n",
        "In your own words describe the results of the notebook. Which model(s) performed the best on the dataset? Why do you think that is? Did anything surprise you about the exercise?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsrnngtS3ur1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}